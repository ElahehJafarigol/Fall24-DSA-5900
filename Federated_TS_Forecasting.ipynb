{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIdkfHmSe2P4wU307Gq1VP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biniwollo/Fall24-DSA-5900/blob/main/Federated_TS_Forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IItqKtYvvlBN",
        "outputId": "3cbd014a-8a3f-416b-a500-dd23c5523c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.1\n"
          ]
        }
      ],
      "source": [
        "  #import TensorFlow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_federated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "xcF1pxsdpTov",
        "outputId": "1e6ef64f-e7c1-4105-965c-22b63ddde2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_federated\n",
            "  Downloading tensorflow_federated-0.87.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.4.0)\n",
            "Collecting attrs~=23.1 (from tensorflow_federated)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (5.5.0)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.1.8)\n",
            "Collecting dp-accounting==0.4.3 (from tensorflow_federated)\n",
            "  Downloading dp_accounting-0.4.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting google-vizier==0.1.11 (from tensorflow_federated)\n",
            "  Downloading google_vizier-0.1.11-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: grpcio~=1.46 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.67.1)\n",
            "Collecting jaxlib==0.4.14 (from tensorflow_federated)\n",
            "  Downloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting jax==0.4.14 (from tensorflow_federated)\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ml-dtypes==0.2.*,>=0.2.0 (from tensorflow_federated)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy~=1.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.26.4)\n",
            "Collecting portpicker~=1.6 (from tensorflow_federated)\n",
            "  Downloading portpicker-1.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting scipy~=1.9.3 (from tensorflow_federated)\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-optimization==0.7.5 (from tensorflow_federated)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
            "Collecting tensorflow-privacy==0.9.0 (from tensorflow_federated)\n",
            "  Downloading tensorflow_privacy-0.9.0-py3-none-any.whl.metadata (763 bytes)\n",
            "Collecting tensorflow==2.14.*,>=2.14.0 (from tensorflow_federated)\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (4.66.6)\n",
            "Collecting typing-extensions==4.5.*,>=4.5.0 (from tensorflow_federated)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting googleapis-common-protos==1.61.0 (from tensorflow_federated)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow_federated) (1.3.0)\n",
            "Collecting attrs~=23.1 (from tensorflow_federated)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow_federated) (4.25.5)\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow_federated)\n",
            "  Downloading grpcio_tools-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting sqlalchemy<=1.4.20,>=1.4 (from google-vizier==0.1.11->tensorflow_federated)\n",
            "  Downloading SQLAlchemy-1.4.20.tar.gz (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow_federated) (3.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (18.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.5.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.37.1)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting packaging (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading packaging-22.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow_federated) (1.5.2)\n",
            "Collecting tensorflow-probability~=0.22.0 (from tensorflow-privacy==0.9.0->tensorflow_federated)\n",
            "  Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow_federated) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow_federated) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker~=1.6->tensorflow_federated) (5.9.5)\n",
            "Collecting numpy~=1.25 (from tensorflow_federated)\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.45.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow_federated)\n",
            "  Downloading grpcio_tools-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.67.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_tools-1.65.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_tools-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow_federated) (3.1.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow_federated) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow_federated) (3.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.2.2)\n",
            "Downloading tensorflow_federated-0.87.0-py3-none-manylinux_2_31_x86_64.whl (71.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_vizier-0.1.11-py3-none-any.whl (721 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl (73.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_privacy-0.9.0-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n",
            "Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.22.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: jax, sqlalchemy\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535358 sha256=9f364ab5981b50ae008f67ed9ff88f9f91f8465e49e3713eb1b88d6a7ae8aede\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/52/e7/dfa571c9f9b879e3facaa1584f52be04c4c3d1e14054ef40ab\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.20-cp310-cp310-linux_x86_64.whl size=1529848 sha256=b552f2939a6187799d96e82397afbef674edec59230c654bc0c18932d193cc51\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/42/20/a958989c470cc1a6fe1d1279b0193f0e508161327fc3d951d9\n",
            "Successfully built jax sqlalchemy\n",
            "Installing collected packages: wrapt, typing-extensions, tensorflow-estimator, sqlalchemy, portpicker, packaging, numpy, keras, grpcio-tools, googleapis-common-protos, attrs, tensorflow-probability, tensorflow-model-optimization, scipy, ml-dtypes, google-vizier, jaxlib, jax, google-auth-oauthlib, dp-accounting, tensorboard, tensorflow, tensorflow-privacy, tensorflow_federated\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: portpicker\n",
            "    Found existing installation: portpicker 1.5.2\n",
            "    Uninstalling portpicker-1.5.2:\n",
            "      Successfully uninstalled portpicker-1.5.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.66.0\n",
            "    Uninstalling googleapis-common-protos-1.66.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.66.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.2.0\n",
            "    Uninstalling attrs-24.2.0:\n",
            "      Successfully uninstalled attrs-24.2.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.24.0\n",
            "    Uninstalling tensorflow-probability-0.24.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.24.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.20 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "chex 0.1.87 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "chex 0.1.87 requires jaxlib>=0.4.27, but you have jaxlib 0.4.14 which is incompatible.\n",
            "flax 0.8.5 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\n",
            "langchain-core 0.3.17 requires packaging<25,>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "langchain-core 0.3.17 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "openai 1.54.4 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "optax 0.2.4 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, but you have jaxlib 0.4.14 which is incompatible.\n",
            "orbax-checkpoint 0.6.4 requires jax>=0.4.26, but you have jax 0.4.14 which is incompatible.\n",
            "pydantic 2.9.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.23.4 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "sphinx 8.1.3 requires packaging>=23.0, but you have packaging 22.0 which is incompatible.\n",
            "tensorstore 0.1.67 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.1 which is incompatible.\n",
            "torch 2.5.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.4.1 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "xarray 2024.10.0 requires packaging>=23.1, but you have packaging 22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 dp-accounting-0.4.3 google-auth-oauthlib-1.0.0 google-vizier-0.1.11 googleapis-common-protos-1.61.0 grpcio-tools-1.62.3 jax-0.4.14 jaxlib-0.4.14 keras-2.14.0 ml-dtypes-0.2.0 numpy-1.25.2 packaging-22.0 portpicker-1.6.0 scipy-1.9.3 sqlalchemy-1.4.20 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-model-optimization-0.7.5 tensorflow-privacy-0.9.0 tensorflow-probability-0.22.1 tensorflow_federated-0.87.0 typing-extensions-4.5.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "jax",
                  "jaxlib",
                  "keras",
                  "ml_dtypes",
                  "numpy",
                  "portpicker",
                  "tensorflow",
                  "wrapt"
                ]
              },
              "id": "4274179e092644a1ac76a5b6399f6701"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import numpy as np\n",
        "import collections\n",
        "import tensorflow_federated as tff"
      ],
      "metadata": {
        "id": "STY_zN-pvtpX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the companies and their ticker symbols\n",
        "companies = {\n",
        "    'John Deere': 'DE',\n",
        "    'Archer-Daniels-Midland': 'ADM',\n",
        "    'Bunge Ltd': 'BG',\n",
        "    'The Mosaic Company': 'MOS',\n",
        "    'Corteva': 'CTVA'\n",
        "}\n",
        "\n",
        "# Set up directory in the default Colab environment\n",
        "base_dir = '/content/FinancialData'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "stock_data_dict = {}\n",
        "\n",
        "# Loop through each company and download the stock data\n",
        "for company, ticker in companies.items():\n",
        "    print(f\"Downloading data for {company} ({ticker})...\")\n",
        "    stock_data = yf.download(ticker, start='2019-09-16', end='2024-09-13')\n",
        "    file_path = os.path.join(base_dir, f\"{ticker}_stock_data.csv\")\n",
        "    stock_data.to_csv(file_path)\n",
        "    stock_data.columns = stock_data.columns.droplevel(1)\n",
        "    stock_data_dict[ticker] = stock_data\n",
        "    print(f\"Data for {company} ({ticker}) saved successfully at {file_path}\")\n",
        "\n",
        "# Combine all data into a single CSV (optional)\n",
        "combined_file_path = os.path.join(base_dir, \"combined_stock_data.csv\")\n",
        "combined_stock_data = pd.concat([pd.read_csv(os.path.join(base_dir, f\"{ticker}_stock_data.csv\")) for ticker in companies.values()])\n",
        "combined_stock_data.to_csv(combined_file_path, index=False)\n",
        "print(f\"Combined stock data saved at: {combined_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQWTODOhvvJ1",
        "outputId": "09376fe8-b216-4bfc-c050-d43f7bf81783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for John Deere (DE)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for John Deere (DE) saved successfully at /content/FinancialData/DE_stock_data.csv\n",
            "Downloading data for Archer-Daniels-Midland (ADM)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for Archer-Daniels-Midland (ADM) saved successfully at /content/FinancialData/ADM_stock_data.csv\n",
            "Downloading data for Bunge Ltd (BG)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for Bunge Ltd (BG) saved successfully at /content/FinancialData/BG_stock_data.csv\n",
            "Downloading data for The Mosaic Company (MOS)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for The Mosaic Company (MOS) saved successfully at /content/FinancialData/MOS_stock_data.csv\n",
            "Downloading data for Corteva (CTVA)...\n",
            "Data for Corteva (CTVA) saved successfully at /content/FinancialData/CTVA_stock_data.csv\n",
            "Combined stock data saved at: /content/FinancialData/combined_stock_data.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDF(df):\n",
        "  column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "  n = len(df)\n",
        "  train_df = df[0:int(n*0.7)]\n",
        "  val_df = df[int(n*0.7):int(n*0.9)]\n",
        "  test_df = df[int(n*0.9):]\n",
        "\n",
        "  num_features = df.shape[1]\n",
        "\n",
        "  train_mean = train_df.mean()\n",
        "  train_std = train_df.std()\n",
        "\n",
        "  train_df = (train_df - train_mean) / train_std\n",
        "  val_df = (val_df - train_mean) / train_std\n",
        "  test_df = (test_df - train_mean) / train_std\n",
        "\n",
        "  return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "qjgvk0lYvyGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowGenerator():\n",
        "  def __init__(self, input_width, label_width, shift,\n",
        "               train_df, val_df, test_df,\n",
        "               label_columns=None):\n",
        "    # Store the raw data.\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "    self.test_df = test_df\n",
        "\n",
        "    # Work out the label column indices.\n",
        "    self.label_columns = label_columns\n",
        "    if label_columns is not None:\n",
        "      self.label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)}\n",
        "    self.column_indices = {name: i for i, name in\n",
        "                           enumerate(train_df.columns)}\n",
        "\n",
        "    # Work out the window parameters.\n",
        "    self.input_width = input_width\n",
        "    self.label_width = label_width\n",
        "    self.shift = shift\n",
        "\n",
        "    self.total_window_size = input_width + shift\n",
        "\n",
        "    self.input_slice = slice(0, input_width)\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "    self.label_start = self.total_window_size - self.label_width\n",
        "    self.labels_slice = slice(self.label_start, None)\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "  def __repr__(self):\n",
        "    return '\\n'.join([\n",
        "        f'Total window size: {self.total_window_size}',\n",
        "        f'Input indices: {self.input_indices}',\n",
        "        f'Label indices: {self.label_indices}',\n",
        "        f'Label column name(s): {self.label_columns}'])"
      ],
      "metadata": {
        "id": "8u5FPEjawOsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_window(self, features):\n",
        "  inputs = features[:, self.input_slice, :]\n",
        "  labels = features[:, self.labels_slice, :]\n",
        "  if self.label_columns is not None:\n",
        "    labels = tf.stack(\n",
        "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "        axis=-1)\n",
        "\n",
        "  # Slicing doesn't preserve static shape information, so set the shapes\n",
        "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "  inputs.set_shape([None, self.input_width, None])\n",
        "  labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "  return inputs, labels\n",
        "\n",
        "WindowGenerator.split_window = split_window"
      ],
      "metadata": {
        "id": "TtKCG-8vwqbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(self, model=None, plot_col='Close', max_subplots=3):\n",
        "  inputs, labels = self.example\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  plot_col_index = self.column_indices[plot_col]\n",
        "  max_n = min(max_subplots, len(inputs))\n",
        "  for n in range(max_n):\n",
        "    plt.subplot(max_n, 1, n+1)\n",
        "    plt.ylabel(f'{plot_col} [normalized]')\n",
        "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
        "             label='Inputs', marker='.', zorder=-10)\n",
        "\n",
        "    if self.label_columns:\n",
        "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
        "    else:\n",
        "      label_col_index = plot_col_index\n",
        "\n",
        "    if label_col_index is None:\n",
        "      continue\n",
        "\n",
        "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
        "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "    if model is not None:\n",
        "      predictions = model(inputs)\n",
        "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
        "                  marker='X', edgecolors='k', label='Predictions',\n",
        "                  c='#ff7f0e', s=64)\n",
        "\n",
        "    if n == 0:\n",
        "      plt.legend()\n",
        "\n",
        "  plt.xlabel('Day')\n",
        "  plt.show()\n",
        "\n",
        "WindowGenerator.plot = plot"
      ],
      "metadata": {
        "id": "BydWDsyBww0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(self, data):\n",
        "  data = np.array(data, dtype=np.float32)\n",
        "  ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "      data=data,\n",
        "      targets=None,\n",
        "      sequence_length=self.total_window_size,\n",
        "      sequence_stride=1,\n",
        "      shuffle=True,\n",
        "      batch_size=32,)\n",
        "\n",
        "  ds = ds.map(self.split_window)\n",
        "\n",
        "  return ds\n",
        "\n",
        "WindowGenerator.make_dataset = make_dataset"
      ],
      "metadata": {
        "id": "zf7QdtMHw0Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@property\n",
        "def train(self):\n",
        "  return self.make_dataset(self.train_df)\n",
        "\n",
        "@property\n",
        "def val(self):\n",
        "  return self.make_dataset(self.val_df)\n",
        "\n",
        "@property\n",
        "def test(self):\n",
        "  return self.make_dataset(self.test_df)\n",
        "\n",
        "@property\n",
        "def example(self):\n",
        "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "  result = getattr(self, '_example', None)\n",
        "  if result is None:\n",
        "    # No example batch was found, so get one from the `.train` dataset\n",
        "    result = next(iter(self.train))\n",
        "    # And cache it for next time\n",
        "    self._example = result\n",
        "  return result\n",
        "\n",
        "WindowGenerator.train = train\n",
        "WindowGenerator.val = val\n",
        "WindowGenerator.test = test\n",
        "WindowGenerator.example = example"
      ],
      "metadata": {
        "id": "Lt8cTOnFw6TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiStepLastBaseline(tf.keras.Model):\n",
        "  def call(self, inputs):\n",
        "    return tf.tile(inputs[:, -1:, :], [1, 50, 1])\n",
        "\n",
        "def Baseline1(window, multi_val_performance, multi_performance):\n",
        "  last_baseline = MultiStepLastBaseline()\n",
        "  last_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                      metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "  multi_val_performance['Baseline'] = last_baseline.evaluate(window.val, return_dict=True)\n",
        "  multi_performance['Baseline'] = last_baseline.evaluate(window.test, verbose=0, return_dict=True)\n",
        "  window.plot(last_baseline)"
      ],
      "metadata": {
        "id": "AWInN1JgxMIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RepeatBaseline(tf.keras.Model):\n",
        "  def call(self, inputs):\n",
        "    return inputs\n",
        "\n",
        "def Baseline2(window, multi_val_performance, multi_performance):\n",
        "  repeat_baseline = RepeatBaseline()\n",
        "  repeat_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                          metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "  multi_val_performance['Repeat'] = repeat_baseline.evaluate(window.val, return_dict=True)\n",
        "  multi_performance['Repeat'] = repeat_baseline.evaluate(window.test, verbose=0, return_dict=True)\n",
        "  window.plot(repeat_baseline)\n"
      ],
      "metadata": {
        "id": "eTqC3KoB7mzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_EPOCHS = 20\n",
        "\n",
        "def compile_and_fit(model, window, patience=2):\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    mode='min')\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
        "                      validation_data=window.val,\n",
        "                      callbacks=[early_stopping])\n",
        "  return history"
      ],
      "metadata": {
        "id": "ZFKm4sa98oZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LinearModel(window, outSteps, multi_val_performance, multi_performance):\n",
        "  num_features = 1\n",
        "  linear_model = tf.keras.Sequential([\n",
        "      # Take the last time-step.\n",
        "      # Shape [batch, time, features] => [batch, 1, features]\n",
        "      tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
        "      # Shape => [batch, 1, out_steps*features]\n",
        "      tf.keras.layers.Dense(outSteps*num_features,\n",
        "                            kernel_initializer=tf.initializers.zeros()),\n",
        "      # Shape => [batch, out_steps, features]\n",
        "      tf.keras.layers.Reshape([outSteps, num_features])\n",
        "  ])\n",
        "\n",
        "  history = compile_and_fit(linear_model, window)\n",
        "\n",
        "  multi_val_performance['Linear'] = linear_model.evaluate(window.val, return_dict=True)\n",
        "  multi_performance['Linear'] = linear_model.evaluate(window.test, verbose=0, return_dict=True)\n",
        "  window.plot(linear_model)"
      ],
      "metadata": {
        "id": "pTEXUvtD8s21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DenseModel(window, outSteps, multi_val_performance, multi_performance):\n",
        "  num_features = 1\n",
        "  multi_dense_model = tf.keras.Sequential([\n",
        "      # Take the last time step.\n",
        "      # Shape [batch, time, features] => [batch, 1, features]\n",
        "      tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
        "      # Shape => [batch, 1, dense_units]\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      # Shape => [batch, out_steps*features]\n",
        "      tf.keras.layers.Dense(outSteps*num_features,\n",
        "                          kernel_initializer=tf.initializers.zeros()),\n",
        "      # Shape => [batch, out_steps, features]\n",
        "      tf.keras.layers.Reshape([outSteps, num_features])])\n",
        "\n",
        "  history = compile_and_fit(multi_dense_model, window)\n",
        "\n",
        "  multi_val_performance['Dense'] = multi_dense_model.evaluate(window.val, return_dict=True)\n",
        "  multi_performance['Dense'] = multi_dense_model.evaluate(window.test, verbose=0, return_dict=True)\n",
        "  window.plot(multi_dense_model)"
      ],
      "metadata": {
        "id": "UxMyzvpJ-H1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ConvModel(window, outSteps, convWidth, multi_val_performance, multi_performance):\n",
        "  num_features=1\n",
        "  multi_conv_model = tf.keras.Sequential([\n",
        "      # Shape [batch, time, features] => [batch, convWidth, features]\n",
        "      tf.keras.layers.Lambda(lambda x: x[:, -convWidth:, :]),\n",
        "      # Shape => [batch, 1, conv_units]\n",
        "      tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(convWidth)),\n",
        "      # Shape => [batch, 1,  out_steps*features]\n",
        "      tf.keras.layers.Dense(outSteps*num_features,\n",
        "                            kernel_initializer=tf.initializers.zeros()),\n",
        "      # Shape => [batch, out_steps, features]\n",
        "      tf.keras.layers.Reshape([outSteps, num_features])\n",
        "  ])\n",
        "\n",
        "  history = compile_and_fit(multi_conv_model, window)\n",
        "\n",
        "  multi_val_performance['Conv'] = multi_conv_model.evaluate(window.val, return_dict=True)\n",
        "  multi_performance['Conv'] = multi_conv_model.evaluate(window.test, verbose=0, return_dict=True)\n",
        "  window.plot(multi_conv_model)"
      ],
      "metadata": {
        "id": "HmCRFQdH_Bjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM(window, outSteps, multi_val_performance, multi_performance):\n",
        "  num_features=1\n",
        "  multi_lstm_model = tf.keras.Sequential([\n",
        "    # Shape [batch, time, features] => [batch, lstm_units].\n",
        "    # Adding more `lstm_units` just overfits more quickly.\n",
        "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
        "    # Shape => [batch, out_steps*features].\n",
        "    tf.keras.layers.Dense(outSteps*num_features,\n",
        "                          kernel_initializer=tf.initializers.zeros()),\n",
        "    # Shape => [batch, out_steps, features].\n",
        "    tf.keras.layers.Reshape([outSteps, num_features])\n",
        "  ])\n",
        "\n",
        "  history = compile_and_fit(multi_lstm_model, window)\n",
        "\n",
        "  multi_val_performance['LSTM'] = multi_lstm_model.evaluate(window.val, return_dict=True)\n",
        "  multi_performance['LSTM'] = multi_lstm_model.evaluate(window.test, verbose=0, return_dict=True)\n",
        "  window.plot(multi_lstm_model)"
      ],
      "metadata": {
        "id": "Eg2pZroB_0dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedBack(tf.keras.Model):\n",
        "  def __init__(self, units, out_steps):\n",
        "    super().__init__()\n",
        "    self.out_steps = out_steps\n",
        "    self.units = units\n",
        "    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
        "    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
        "    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(6)\n",
        "\n",
        "def warmup(self, inputs):\n",
        "  # inputs.shape => (batch, time, features)\n",
        "  # x.shape => (batch, lstm_units)\n",
        "  x, *state = self.lstm_rnn(inputs)\n",
        "\n",
        "  # predictions.shape => (batch, features)\n",
        "  prediction = self.dense(x)\n",
        "  return prediction, state\n",
        "\n",
        "FeedBack.warmup = warmup\n",
        "\n",
        "def call(self, inputs, training=None):\n",
        "  # Use a TensorArray to capture dynamically unrolled outputs.\n",
        "  predictions = []\n",
        "  # Initialize the LSTM state.\n",
        "  prediction, state = self.warmup(inputs)\n",
        "\n",
        "  # Insert the first prediction.\n",
        "  predictions.append(prediction)\n",
        "\n",
        "  # Run the rest of the prediction steps.\n",
        "  for n in range(1, self.out_steps):\n",
        "    # Use the last prediction as input.\n",
        "    x = prediction\n",
        "    # Execute one lstm step.\n",
        "    x, state = self.lstm_cell(x, states=state,\n",
        "                              training=training)\n",
        "    # Convert the lstm output to a prediction.\n",
        "    prediction = self.dense(x)\n",
        "    # Add the prediction to the output.\n",
        "    predictions.append(prediction)\n",
        "\n",
        "  # predictions.shape => (time, batch, features)\n",
        "  predictions = tf.stack(predictions)\n",
        "  # predictions.shape => (batch, time, features)\n",
        "  predictions = tf.transpose(predictions, [1, 0, 2])\n",
        "  return predictions\n",
        "\n",
        "FeedBack.call = call\n",
        "\n",
        "def AR_LSTM(window, outSteps, multi_val_performance, multi_performance):\n",
        "  feedback_model = FeedBack(units=32, out_steps=outSteps)\n",
        "  history = compile_and_fit(feedback_model, window)\n",
        "\n",
        "  multi_val_performance['AR LSTM'] = feedback_model.evaluate(window.val, return_dict=True)\n",
        "  multi_performance['AR LSTM'] = feedback_model.evaluate(window.test, verbose=0, return_dict=True)\n",
        "  window.plot(feedback_model)"
      ],
      "metadata": {
        "id": "TuyCMtU4APjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printModelSummary(multi_val_performance, multi_performance):\n",
        "  x = np.arange(len(multi_performance))\n",
        "  width = 0.3\n",
        "\n",
        "  metric_name = 'mean_absolute_error'\n",
        "  val_mae = [v[metric_name] for v in multi_val_performance.values()]\n",
        "  test_mae = [v[metric_name] for v in multi_performance.values()]\n",
        "\n",
        "  plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
        "  plt.bar(x + 0.17, test_mae, width, label='Test')\n",
        "  plt.xticks(ticks=x, labels=multi_performance.keys(),\n",
        "            rotation=45)\n",
        "  plt.ylabel(f'MAE (average over all times and outputs)')\n",
        "  _ = plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "fwNEwIbXBUN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "OUT_STEPS = 50\n",
        "CONV_WIDTH = 3\n",
        "window = WindowGenerator(input_width=50, label_width=OUT_STEPS, shift=OUT_STEPS, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=['Close'])\n",
        "window\n",
        "\n",
        "client_train_dataset = dict()\n",
        "client_test_dataset = dict()\n",
        "client_ids = []\n",
        "for ticker, df in stock_data_dict.items():\n",
        "  train_df, val_df, test_df = splitDF(df)\n",
        "  OUT_STEPS = 50\n",
        "  CONV_WIDTH = 3\n",
        "  window = WindowGenerator(input_width=50, label_width=OUT_STEPS, shift=OUT_STEPS, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=['Close'])\n",
        "  client_train_dataset[ticker] = window.train\n",
        "  client_test_dataset[ticker] = window.test\n",
        "  client_ids.append(ticker)\n",
        "\n",
        "client_train_dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CkbJVOSxfmp",
        "outputId": "fcd276e6-0f90-4538-b9da-f0044e761866"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DE': <_MapDataset element_spec=(TensorSpec(shape=(None, 50, 6), dtype=tf.float32, name=None), TensorSpec(shape=(None, 50, 1), dtype=tf.float32, name=None))>,\n",
              " 'ADM': <_MapDataset element_spec=(TensorSpec(shape=(None, 50, 6), dtype=tf.float32, name=None), TensorSpec(shape=(None, 50, 1), dtype=tf.float32, name=None))>,\n",
              " 'BG': <_MapDataset element_spec=(TensorSpec(shape=(None, 50, 6), dtype=tf.float32, name=None), TensorSpec(shape=(None, 50, 1), dtype=tf.float32, name=None))>,\n",
              " 'MOS': <_MapDataset element_spec=(TensorSpec(shape=(None, 50, 6), dtype=tf.float32, name=None), TensorSpec(shape=(None, 50, 1), dtype=tf.float32, name=None))>,\n",
              " 'CTVA': <_MapDataset element_spec=(TensorSpec(shape=(None, 50, 6), dtype=tf.float32, name=None), TensorSpec(shape=(None, 50, 1), dtype=tf.float32, name=None))>}"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(client):\n",
        "  return client_train_dataset[client]\n",
        "\n",
        "client_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(client_ids, create_dataset)\n",
        "client_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoF8G7YHL403",
        "outputId": "f3b5e84f-0848-4b1a-dba5-302af9634157"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_federated.python.simulation.datasets.client_data.ConcreteClientData at 0x7fa0c018e2f0>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(client_data.client_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5nH2cAZVp9E",
        "outputId": "fc8ab758-b352-4854-8c80-0a435eb37bfd"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_dataset = client_data.create_tf_dataset_for_client(\n",
        "    client_data.client_ids[0])\n",
        "\n",
        "example_element = next(iter(example_dataset))\n",
        "example_element"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFG2RikiWniP",
        "outputId": "9d9cfa96-6db1-43d5-a531-b7d6f7f9be81"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(32, 50, 6), dtype=float32, numpy=\n",
              " array([[[-0.6470073 , -0.64284134, -0.6628972 , -0.6248717 ,\n",
              "          -0.63524795, -0.60462964],\n",
              "         [-0.64004076, -0.63561296, -0.6632033 , -0.6309277 ,\n",
              "          -0.6455614 , -0.5116219 ],\n",
              "         [-0.62760085, -0.62270504, -0.6317782 , -0.6037803 ,\n",
              "          -0.63266957, -0.81410986],\n",
              "         ...,\n",
              "         [-0.3682542 , -0.3536008 , -0.38711154, -0.34848994,\n",
              "          -0.36823204, -0.25180805],\n",
              "         [-0.3068507 , -0.2898872 , -0.31762955, -0.31695727,\n",
              "          -0.3479145 ,  1.8747263 ],\n",
              "         [-0.3362098 , -0.32035   , -0.31201804, -0.30630723,\n",
              "          -0.33842596,  0.50441194]],\n",
              " \n",
              "        [[-1.272003  , -1.2657259 , -1.2811955 , -1.2465479 ,\n",
              "          -1.2688047 , -0.09570089],\n",
              "         [-1.3077176 , -1.3033137 , -1.2973162 , -1.2793337 ,\n",
              "          -1.2750959 , -0.09363405],\n",
              "         [-1.3214538 , -1.3177705 , -1.3153753 , -1.2978148 ,\n",
              "          -1.2960323 , -0.54274607],\n",
              "         ...,\n",
              "         [-1.636117  , -1.6489362 , -1.657276  , -1.7518034 ,\n",
              "          -1.7469374 ,  0.6022829 ],\n",
              "         [-1.6751674 , -1.6900349 , -1.6672748 , -1.6993879 ,\n",
              "          -1.6886663 ,  0.6935885 ],\n",
              "         [-1.6072809 , -1.6268377 , -1.6414614 , -1.6743288 ,\n",
              "          -1.6709272 ,  0.6309754 ]],\n",
              " \n",
              "        [[ 0.7160767 ,  0.6947285 ,  0.65001744,  0.6701652 ,\n",
              "           0.6572368 , -0.65168065],\n",
              "         [ 0.75598216,  0.7353112 ,  0.72225416,  0.7109908 ,\n",
              "           0.6763168 , -0.3654842 ],\n",
              "         [ 0.70226705,  0.6806848 ,  0.6618529 ,  0.7033685 ,\n",
              "           0.67796695, -0.01168996],\n",
              "         ...,\n",
              "         [ 0.72285944,  0.68925554,  0.72949815,  0.718404  ,\n",
              "           0.753874  , -1.1474789 ],\n",
              "         [ 0.90154594,  0.8703795 ,  0.8269364 ,  0.71161723,\n",
              "           0.6788952 , -0.41897887],\n",
              "         [ 0.9303768 ,  0.8996032 ,  0.8658096 ,  0.8895371 ,\n",
              "           0.8553589 , -0.81751406]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0.51273555,  0.52114284,  0.51482844,  0.5229425 ,\n",
              "           0.5557522 , -0.4285836 ],\n",
              "         [ 0.5657554 ,  0.5755628 ,  0.5644146 ,  0.58047426,\n",
              "           0.557093  , -0.22931597],\n",
              "         [ 0.63446873,  0.6460915 ,  0.65695536,  0.6706871 ,\n",
              "           0.62866855, -0.30785584],\n",
              "         ...,\n",
              "         [ 0.9665473 ,  0.9745722 ,  0.98589814,  0.9937416 ,\n",
              "           0.9701476 , -0.4009852 ],\n",
              "         [ 0.9617033 ,  0.96961576,  0.99120384,  0.9648192 ,\n",
              "           0.9853084 ,  0.8000915 ],\n",
              "         [ 0.9071095 ,  0.91375023,  0.9703898 ,  0.929632  ,\n",
              "           0.98365825,  0.08241201]],\n",
              " \n",
              "        [[-1.3925592 , -1.3774568 , -1.3973049 , -1.379257  ,\n",
              "          -1.3950417 , -0.62760806],\n",
              "         [-1.4017001 , -1.3871635 , -1.3935299 , -1.3700688 ,\n",
              "          -1.3777151 , -0.90711766],\n",
              "         [-1.4052986 , -1.3909842 , -1.3863878 , -1.3786305 ,\n",
              "          -1.3810154 ,  1.4718143 ],\n",
              "         ...,\n",
              "         [-1.2874869 , -1.2741934 , -1.2815017 , -1.256154  ,\n",
              "          -1.2640606 , -0.53411394],\n",
              "         [-1.2754709 , -1.261492  , -1.2820119 , -1.2499936 ,\n",
              "          -1.2649888 , -0.21314599],\n",
              "         [-1.2750801 , -1.2610791 , -1.2676256 , -1.2353756 ,\n",
              "          -1.2559129 ,  1.2823945 ]],\n",
              " \n",
              "        [[-1.2991815 , -1.2943299 , -1.2670134 , -1.2686834 ,\n",
              "          -1.2441555 ,  1.0076265 ],\n",
              "         [-1.286328  , -1.2808022 , -1.3012954 , -1.2670128 ,\n",
              "          -1.2856157 , -0.52426606],\n",
              "         [-1.2968267 , -1.2918515 , -1.3011932 , -1.2684748 ,\n",
              "          -1.2773649 , -0.84170824],\n",
              "         ...,\n",
              "         [-1.826759  , -1.8495767 , -1.8553151 , -1.9699225 ,\n",
              "          -1.9165941 ,  0.87972564],\n",
              "         [-1.9055473 , -1.9324973 , -1.8566414 , -1.9272176 ,\n",
              "          -1.8468751 ,  2.945957  ],\n",
              "         [-1.9102569 , -1.9374539 , -1.9080642 , -1.9671034 ,\n",
              "          -1.9437187 ,  1.7006255 ]]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(32, 50, 1), dtype=float32, numpy=\n",
              " array([[[-0.30475715],\n",
              "         [-0.318388  ],\n",
              "         [-0.30527365],\n",
              "         ...,\n",
              "         [ 0.45732614],\n",
              "         [ 0.40455875],\n",
              "         [ 0.5272353 ]],\n",
              " \n",
              "        [[-1.6585395 ],\n",
              "         [-1.7049048 ],\n",
              "         [-1.6459414 ],\n",
              "         ...,\n",
              "         [-1.3420374 ],\n",
              "         [-1.3833429 ],\n",
              "         [-1.4226861 ]],\n",
              " \n",
              "        [[ 0.95887625],\n",
              "         [ 0.9542293 ],\n",
              "         [ 0.9894421 ],\n",
              "         ...,\n",
              "         [ 1.2961344 ],\n",
              "         [ 1.28777   ],\n",
              "         [ 1.1974146 ]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0.9771538 ],\n",
              "         [ 0.947827  ],\n",
              "         [ 0.8442539 ],\n",
              "         ...,\n",
              "         [ 0.8956791 ],\n",
              "         [ 0.85705847],\n",
              "         [ 0.8958856 ]],\n",
              " \n",
              "        [[-1.3394558 ],\n",
              "         [-1.3498855 ],\n",
              "         [-1.3783863 ],\n",
              "         ...,\n",
              "         [-1.3415211 ],\n",
              "         [-1.3560812 ],\n",
              "         [-1.3479234 ]],\n",
              " \n",
              "        [[-1.7841078 ],\n",
              "         [-1.7583952 ],\n",
              "         [-1.6489362 ],\n",
              "         ...,\n",
              "         [-1.5451565 ],\n",
              "         [-1.5058131 ],\n",
              "         [-1.4500508 ]]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      client_data.create_tf_dataset_for_client(x)\n",
        "      for x in client_ids\n",
        "  ]"
      ],
      "metadata": {
        "id": "1M77q56kW9xI"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "federated_train_data = make_federated_data(client_data, client_ids)\n",
        "\n",
        "print(f'Number of client datasets: {len(federated_train_data)}')\n",
        "print(f'First dataset: {federated_train_data[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvjakgGcXJeG",
        "outputId": "7fd92e62-3c61-4fa9-c401-cfa9836c446f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of client datasets: 5\n",
            "First dataset: <_MapDataset element_spec=(TensorSpec(shape=(None, 50, 6), dtype=tf.float32, name=None), TensorSpec(shape=(None, 50, 1), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn():\n",
        "  # We _must_ create a new model here, and _not_ capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "  num_features = 1\n",
        "  linear_model = tf.keras.Sequential([\n",
        "      # Take the last time-step.\n",
        "      tf.keras.layers.InputLayer(input_shape=(50,6)),\n",
        "      # Shape [batch, time, features] => [batch, 1, features]\n",
        "      tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
        "      # Shape => [batch, 1, out_steps*features]\n",
        "      tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
        "                            kernel_initializer=tf.initializers.zeros()),\n",
        "      # Shape => [batch, out_steps, features]\n",
        "      tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
        "  ])\n",
        "\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      linear_model,\n",
        "      input_spec=client_train_dataset['DE'].element_spec,\n",
        "      loss=tf.keras.losses.MeanSquaredError(),\n",
        "      metrics=[tf.keras.metrics.MeanAbsoluteError()])"
      ],
      "metadata": {
        "id": "sWAtCHkfsGOR"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=0.02),\n",
        "    server_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=1.0))"
      ],
      "metadata": {
        "id": "egqYb3ZgF-XW"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_process.initialize.type_signature.formatted_representation())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oogcoyPRHr79",
        "outputId": "85458301-3e6b-4406-f1da-a3be1a30b50b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "( -> <\n",
            "  global_model_weights=<\n",
            "    trainable=<\n",
            "      float32[6,50],\n",
            "      float32[50]\n",
            "    >,\n",
            "    non_trainable=<>\n",
            "  >,\n",
            "  distributor=<>,\n",
            "  client_work=<\n",
            "    learning_rate=float32\n",
            "  >,\n",
            "  aggregator=<\n",
            "    value_sum_process=<>,\n",
            "    weight_sum_process=<>\n",
            "  >,\n",
            "  finalizer=<\n",
            "    learning_rate=float32\n",
            "  >\n",
            ">@SERVER)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_state = training_process.initialize()"
      ],
      "metadata": {
        "id": "p_aUGpaaIDva"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = training_process.next(train_state, federated_train_data)\n",
        "train_state = result.state\n",
        "train_metrics = result.metrics\n",
        "print('round  1, metrics={}'.format(train_metrics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAEiHOIoIGQ4",
        "outputId": "2c164919-f3a6-47d8-ac5c-b9714cb8831c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.8007774), ('loss', 0.89618546), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_ROUNDS = 25\n",
        "for round_num in range(2, NUM_ROUNDS):\n",
        "  result = training_process.next(train_state, federated_train_data)\n",
        "  train_state = result.state\n",
        "  train_metrics = result.metrics\n",
        "  print('round {:2d}, metrics={}'.format(round_num, train_metrics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-YQW7HVXgr5",
        "outputId": "a7772cb8-0150-434e-d18a-9f953199e2ee"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "round  2, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.73515666), ('loss', 0.7599916), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  3, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.6759308), ('loss', 0.647527), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  4, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.6224728), ('loss', 0.5546526), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  5, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.57446396), ('loss', 0.4779513), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  6, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.5316165), ('loss', 0.41460234), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  7, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.493596), ('loss', 0.36227703), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  8, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.4599986), ('loss', 0.31905305), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round  9, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.43036163), ('loss', 0.2833433), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 10, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.40424052), ('loss', 0.25383773), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 11, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.38127652), ('loss', 0.22945473), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 12, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.36116177), ('loss', 0.20930141), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 13, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.3436254), ('loss', 0.19264068), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 14, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.32842243), ('loss', 0.178864), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 15, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.31529397), ('loss', 0.16746894), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 16, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.3039617), ('loss', 0.15804079), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 17, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.29418388), ('loss', 0.15023711), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 18, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.28577355), ('loss', 0.14377524), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 19, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.278569), ('loss', 0.13842173), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 20, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.27241758), ('loss', 0.13398394), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 21, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.26717272), ('loss', 0.13030276), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 22, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.26270333), ('loss', 0.1272468), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 23, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.2589029), ('loss', 0.124707595), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "round 24, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_absolute_error', 0.25568217), ('loss', 0.12259562), ('num_examples', 3900), ('num_batches', 125)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_process = tff.learning.algorithms.build_fed_eval(model_fn)\n",
        "print(evaluation_process.next.type_signature.formatted_representation())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXIc-S3nbt8B",
        "outputId": "4c20c73e-59e7-4d1a-be80-7e58f6cf190f"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<\n",
            "  state=<\n",
            "    global_model_weights=<\n",
            "      trainable=<\n",
            "        float32[6,50],\n",
            "        float32[50]\n",
            "      >,\n",
            "      non_trainable=<>\n",
            "    >,\n",
            "    distributor=<>,\n",
            "    client_work=<\n",
            "      <>,\n",
            "      <\n",
            "        mean_absolute_error=<\n",
            "          float32,\n",
            "          float32\n",
            "        >,\n",
            "        loss=<\n",
            "          float32,\n",
            "          float32\n",
            "        >,\n",
            "        num_examples=<\n",
            "          int64\n",
            "        >,\n",
            "        num_batches=<\n",
            "          int64\n",
            "        >\n",
            "      >\n",
            "    >,\n",
            "    aggregator=<\n",
            "      value_sum_process=<>,\n",
            "      weight_sum_process=<>\n",
            "    >,\n",
            "    finalizer=<>\n",
            "  >@SERVER,\n",
            "  client_data={<\n",
            "    float32[?,50,6],\n",
            "    float32[?,50,1]\n",
            "  >*}@CLIENTS\n",
            "> -> <\n",
            "  state=<\n",
            "    global_model_weights=<\n",
            "      trainable=<\n",
            "        float32[6,50],\n",
            "        float32[50]\n",
            "      >,\n",
            "      non_trainable=<>\n",
            "    >,\n",
            "    distributor=<>,\n",
            "    client_work=<\n",
            "      <>,\n",
            "      <\n",
            "        mean_absolute_error=<\n",
            "          float32,\n",
            "          float32\n",
            "        >,\n",
            "        loss=<\n",
            "          float32,\n",
            "          float32\n",
            "        >,\n",
            "        num_examples=<\n",
            "          int64\n",
            "        >,\n",
            "        num_batches=<\n",
            "          int64\n",
            "        >\n",
            "      >\n",
            "    >,\n",
            "    aggregator=<\n",
            "      value_sum_process=<>,\n",
            "      weight_sum_process=<>\n",
            "    >,\n",
            "    finalizer=<>\n",
            "  >@SERVER,\n",
            "  metrics=<\n",
            "    distributor=<>,\n",
            "    client_work=<\n",
            "      eval=<\n",
            "        current_round_metrics=<\n",
            "          mean_absolute_error=float32,\n",
            "          loss=float32,\n",
            "          num_examples=int64,\n",
            "          num_batches=int64\n",
            "        >,\n",
            "        total_rounds_metrics=<\n",
            "          mean_absolute_error=float32,\n",
            "          loss=float32,\n",
            "          num_examples=int64,\n",
            "          num_batches=int64\n",
            "        >\n",
            "      >\n",
            "    >,\n",
            "    aggregator=<\n",
            "      mean_value=<>,\n",
            "      mean_weight=<>\n",
            "    >,\n",
            "    finalizer=<>\n",
            "  >@SERVER\n",
            ">)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_state = evaluation_process.initialize()\n",
        "model_weights = training_process.get_model_weights(train_state)\n",
        "evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)"
      ],
      "metadata": {
        "id": "MbOnFgeZcANV"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_dataset(client):\n",
        "  return client_test_dataset[client]\n",
        "\n",
        "client_test_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(client_ids, create_test_dataset)\n",
        "\n",
        "federated_test_data = make_federated_data(client_test_data, client_ids)\n",
        "\n",
        "len(federated_test_data), federated_test_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKdNZst3cQ2T",
        "outputId": "8a2695f6-dcec-4a40-82f5-658ad0411ac8"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,\n",
              " <_MapDataset element_spec=(TensorSpec(shape=(None, 50, 6), dtype=tf.float32, name=None), TensorSpec(shape=(None, 50, 1), dtype=tf.float32, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_output = evaluation_process.next(evaluation_state, federated_test_data)"
      ],
      "metadata": {
        "id": "sAb_J04kdehl"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str(evaluation_output.metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "lnsko5VldkGc",
        "outputId": "444e282c-8bdb-4a67-b992-b166dcd630e3"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"OrderedDict([('distributor', ()), ('client_work', OrderedDict([('eval', OrderedDict([('current_round_metrics', OrderedDict([('mean_absolute_error', 0.15457316), ('loss', 0.036595188), ('num_examples', 135), ('num_batches', 5)])), ('total_rounds_metrics', OrderedDict([('mean_absolute_error', 0.15457316), ('loss', 0.036595188), ('num_examples', 135), ('num_batches', 5)]))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zha0K-aL1OnN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}