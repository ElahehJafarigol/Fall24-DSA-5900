{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biniwollo/Fall24-DSA-5900/blob/main/SetUp_TFF_for_FiveCompanies_V02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-Step Guide to Setting Up Federated Learning with TensorFlow Federated:\n",
        "1. Install TensorFlow Federated\n",
        "\n",
        "You first need to install the tensorflow-federated library.\n",
        "\n",
        "In Google Colab, or your local Python environment, run:"
      ],
      "metadata": {
        "id": "pVKG_jkpKNHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install TensorFlow Federated"
      ],
      "metadata": {
        "id": "RrsKY-CShBgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import TensorFlow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFc454D2bT1J",
        "outputId": "52086b62-38dd-4209-e66f-e29dbc31b8e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install tensorflow federated\n",
        "!pip install tensorflow-federated\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HSABqPDUgzL7",
        "collapsed": true,
        "outputId": "89a80533-40cf-499d-aef4-7e06cb253c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.87.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.4.0)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (5.5.0)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.1.8)\n",
            "Collecting dp-accounting==0.4.3 (from tensorflow-federated)\n",
            "  Downloading dp_accounting-0.4.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting google-vizier==0.1.11 (from tensorflow-federated)\n",
            "  Downloading google_vizier-0.1.11-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: grpcio~=1.46 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.66.2)\n",
            "Collecting jaxlib==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting jax==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ml-dtypes==0.2.*,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (1.26.4)\n",
            "Collecting portpicker~=1.6 (from tensorflow-federated)\n",
            "  Downloading portpicker-1.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting scipy~=1.9.3 (from tensorflow-federated)\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-optimization==0.7.5 (from tensorflow-federated)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
            "Collecting tensorflow-privacy==0.9.0 (from tensorflow-federated)\n",
            "  Downloading tensorflow_privacy-0.9.0-py3-none-any.whl.metadata (763 bytes)\n",
            "Collecting tensorflow==2.14.*,>=2.14.0 (from tensorflow-federated)\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (4.66.5)\n",
            "Collecting typing-extensions==4.5.*,>=4.5.0 (from tensorflow-federated)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting googleapis-common-protos==1.61.0 (from tensorflow-federated)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow-federated) (1.3.0)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow-federated) (3.20.3)\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.66.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting sqlalchemy<=1.4.20,>=1.4 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading SQLAlchemy-1.4.20.tar.gz (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow-federated) (3.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (18.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.37.1)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting packaging (from tensorflow==2.14.*,>=2.14.0->tensorflow-federated)\n",
            "  Downloading packaging-22.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated) (1.5.2)\n",
            "Collecting tensorflow-probability~=0.22.0 (from tensorflow-privacy==0.9.0->tensorflow-federated)\n",
            "  Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker~=1.6->tensorflow-federated) (5.9.5)\n",
            "Collecting numpy~=1.25 (from tensorflow-federated)\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.44.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Using cached grpcio_tools-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting protobuf>=3.6 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting greenlet!=0.4.17 (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.0.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (5.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow-federated) (3.2.2)\n",
            "Downloading tensorflow_federated-0.87.0-py3-none-manylinux_2_31_x86_64.whl (71.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_vizier-0.1.11-py3-none-any.whl (721 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl (73.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_privacy-0.9.0-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n",
            "Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.22.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jax, sqlalchemy\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535359 sha256=0aae5991b9360846f123c0dc682afd154dfe1b857055ba7e381e9fa694474dbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/52/e7/dfa571c9f9b879e3facaa1584f52be04c4c3d1e14054ef40ab\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.20-cp310-cp310-linux_x86_64.whl size=1529851 sha256=94f0f37a6f0210f999e40665dd414cd81b44ab253ec59b9e13847e03a2a96fb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/42/20/a958989c470cc1a6fe1d1279b0193f0e508161327fc3d951d9\n",
            "Successfully built jax sqlalchemy\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, protobuf, portpicker, packaging, numpy, keras, greenlet, attrs, tensorflow-probability, tensorflow-model-optimization, sqlalchemy, scipy, grpcio-tools, googleapis-common-protos, jaxlib, jax, google-vizier, google-auth-oauthlib, dp-accounting, tensorboard, tensorflow, tensorflow-privacy, tensorflow-federated\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: portpicker\n",
            "    Found existing installation: portpicker 1.5.2\n",
            "    Uninstalling portpicker-1.5.2:\n",
            "      Successfully uninstalled portpicker-1.5.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.2.0\n",
            "    Uninstalling attrs-24.2.0:\n",
            "      Successfully uninstalled attrs-24.2.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.24.0\n",
            "    Uninstalling tensorflow-probability-0.24.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.24.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.65.0\n",
            "    Uninstalling googleapis-common-protos-1.65.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.65.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.87 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "chex 0.1.87 requires jaxlib>=0.4.27, but you have jaxlib 0.4.14 which is incompatible.\n",
            "flax 0.8.5 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.5 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "pydantic 2.9.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.23.4 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-metadata 1.16.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\n",
            "tensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.14.1 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.14.1 which is incompatible.\n",
            "torch 2.4.0+cpu requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 dp-accounting-0.4.3 google-auth-oauthlib-1.0.0 google-vizier-0.1.11 googleapis-common-protos-1.61.0 greenlet-3.1.1 grpcio-tools-1.62.3 jax-0.4.14 jaxlib-0.4.14 keras-2.14.0 numpy-1.25.2 packaging-22.0 portpicker-1.6.0 protobuf-4.25.5 scipy-1.9.3 sqlalchemy-1.4.20 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-federated-0.87.0 tensorflow-model-optimization-0.7.5 tensorflow-privacy-0.9.0 tensorflow-probability-0.22.1 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "jax",
                  "jaxlib",
                  "keras",
                  "numpy",
                  "portpicker",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "a970dc4ba66644059239930f3f27754e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import TensorFlow Federated"
      ],
      "metadata": {
        "id": "DvIJCacmh1E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_federated as tff\n",
        "\n",
        "# Verify TensorFlow Federated version\n",
        "print(f\"TensorFlow Federated Version: {tff.__version__}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWBmERLTUexu",
        "outputId": "08574fc4-d87e-4c05-a076-7fc53c2ffa15"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Federated Version: 0.87.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve Stock Data from Yahoo Finance"
      ],
      "metadata": {
        "id": "VrrFwLHikRLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "ySfLiO05kd88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "6991b1aa-1b6c-4881-c1e3-124683f17417"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.2.44-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.32.3)\n",
            "Collecting multitasking>=0.0.7 (from yfinance)\n",
            "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting lxml>=4.9.1 (from yfinance)\n",
            "  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2024.2)\n",
            "Collecting frozendict>=2.3.4 (from yfinance)\n",
            "  Downloading frozendict-2.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Collecting peewee>=3.16.2 (from yfinance)\n",
            "  Downloading peewee-3.17.6.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Collecting html5lib>=1.1 (from yfinance)\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.8.30)\n",
            "Downloading yfinance-0.2.44-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozendict-2.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
            "Building wheels for collected packages: peewee\n",
            "  Building wheel for peewee (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peewee: filename=peewee-3.17.6-cp310-cp310-linux_x86_64.whl size=728063 sha256=54b7d065c0736760386776f195178e05b584d839d873a07131aba83b4cbd5bdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/b9/b0/83d6e258e8f963f5ff111a2cd8c483ca59372a86e6a2535212\n",
            "Successfully built peewee\n",
            "Installing collected packages: peewee, multitasking, lxml, html5lib, frozendict, yfinance\n",
            "Successfully installed frozendict-2.4.5 html5lib-1.1 lxml-5.3.0 multitasking-0.0.11 peewee-3.17.6 yfinance-0.2.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the companies and their ticker symbols\n",
        "companies = {\n",
        "    'John Deere': 'DE',\n",
        "    'Archer-Daniels-Midland': 'ADM',\n",
        "    'Bunge Ltd': 'BG',\n",
        "    'The Mosaic Company': 'MOS',\n",
        "    'Corteva': 'CTVA'\n",
        "}\n",
        "\n",
        "# Set up directory in the default Colab environment\n",
        "base_dir = '/content/FinancialData'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Loop through each company and download the stock data\n",
        "for company, ticker in companies.items():\n",
        "    print(f\"Downloading data for {company} ({ticker})...\")\n",
        "    stock_data = yf.download(ticker, start='2019-09-16', end='2024-09-13')\n",
        "    file_path = os.path.join(base_dir, f\"{ticker}_stock_data.csv\")\n",
        "    stock_data.to_csv(file_path)\n",
        "    print(f\"Data for {company} ({ticker}) saved successfully at {file_path}\")\n",
        "\n",
        "# Combine all data into a single CSV (optional)\n",
        "combined_file_path = os.path.join(base_dir, \"combined_stock_data.csv\")\n",
        "combined_stock_data = pd.concat([pd.read_csv(os.path.join(base_dir, f\"{ticker}_stock_data.csv\")) for ticker in companies.values()])\n",
        "combined_stock_data.to_csv(combined_file_path, index=False)\n",
        "print(f\"Combined stock data saved at: {combined_file_path}\")\n"
      ],
      "metadata": {
        "id": "Z3FrM1QekVwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b0005394-22fa-45bc-ca0c-adde7dc33fe6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for John Deere (DE)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for John Deere (DE) saved successfully at /content/FinancialData/DE_stock_data.csv\n",
            "Downloading data for Archer-Daniels-Midland (ADM)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for Archer-Daniels-Midland (ADM) saved successfully at /content/FinancialData/ADM_stock_data.csv\n",
            "Downloading data for Bunge Ltd (BG)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for Bunge Ltd (BG) saved successfully at /content/FinancialData/BG_stock_data.csv\n",
            "Downloading data for The Mosaic Company (MOS)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for The Mosaic Company (MOS) saved successfully at /content/FinancialData/MOS_stock_data.csv\n",
            "Downloading data for Corteva (CTVA)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for Corteva (CTVA) saved successfully at /content/FinancialData/CTVA_stock_data.csv\n",
            "Combined stock data saved at: /content/FinancialData/combined_stock_data.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that both TensorFlow (2.14.0) and TensorFlow Federated (0.87.0) are successfully installed and ready to use, we're all set to start working on your project. Let's begin our financial analysis using TensorFlow Federated Learning with the stock data, here's a simple structure to get started:\n",
        "Steps for Using TensorFlow Federated Learning:\n",
        "\n",
        "1. Prepare the Dataset: Organize our stock data for each company into client datasets for federated learning.\n",
        "2. Define the Model: Build a machine learning model (e.g., for stock price prediction or volatility analysis).\n",
        "3. Federated Training: Set up the federated learning process where each company acts as a client in the federated environment.\n",
        "4. Evaluate the Model: Assess the performance of the federated model across all clients.\n",
        "\n",
        "Here's an outline of how you can structure the code:"
      ],
      "metadata": {
        "id": "lYvC2k8Git6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the saved stock data in Colab\n",
        "base_dir = '/content/FinancialData'\n",
        "\n",
        "# Load each company's stock data from the saved CSV files\n",
        "def load_company_data(ticker):\n",
        "    file_path = os.path.join(base_dir, f\"{ticker}_stock_data.csv\")\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to load stock data for all companies and convert it to tf.data.Dataset\n",
        "def create_client_data():\n",
        "    companies = ['DE', 'ADM', 'BG', 'MOS', 'CTVA']  # List of ticker symbols\n",
        "    client_data = []\n",
        "\n",
        "    for ticker in companies:\n",
        "        data = load_company_data(ticker)\n",
        "        # Debug: Check if data is loaded correctly\n",
        "        #print(f\"Data for {ticker} loaded. Sample:\\n\", data.head())\n",
        "\n",
        "        # Use relevant features for the model (e.g., Close price, Volume)\n",
        "        features = data[['Open', 'Volume']].fillna(0).values.astype('float32')  # Changed to float32\n",
        "        labels = data[['Close']].fillna(0).values.astype('float32')  # Predicting Close price, changed to float32\n",
        "\n",
        "        # Debug: Check features and labels\n",
        "        #print(f\"Features shape for {ticker}: {features.shape}\")\n",
        "        #print(f\"Labels shape for {ticker}: {labels.shape}\")\n",
        "\n",
        "        # Create dataset and check batches\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "        dataset = dataset.batch(32)  # Batch the dataset\n",
        "        client_data.append(dataset)\n",
        "\n",
        "        # Debug: Check batched dataset\n",
        "        #print(f\"Batched dataset for {ticker}:\")\n",
        "        #for batch in dataset:\n",
        "            #print(batch)\n",
        "\n",
        "        return client_data\n",
        "\n",
        "\n",
        "clientData = create_client_data()\n",
        "\n",
        "for batch in clientData[0]:\n",
        "  print(batch)\n",
        "  break\n"
      ],
      "metadata": {
        "id": "NKxgPqGO-HUe",
        "collapsed": true,
        "outputId": "d4b902aa-dd15-4c68-d5c1-6fc4cef274cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(32, 2), dtype=float32, numpy=\n",
            "array([[1.6414e+02, 1.1647e+06],\n",
            "       [1.6299e+02, 1.3478e+06],\n",
            "       [1.6347e+02, 1.2159e+06],\n",
            "       [1.6515e+02, 9.8600e+05],\n",
            "       [1.6483e+02, 2.9427e+06],\n",
            "       [1.6289e+02, 1.4123e+06],\n",
            "       [1.6561e+02, 3.1248e+06],\n",
            "       [1.6568e+02, 2.8140e+06],\n",
            "       [1.6517e+02, 1.8088e+06],\n",
            "       [1.6650e+02, 1.5912e+06],\n",
            "       [1.6678e+02, 1.8287e+06],\n",
            "       [1.6960e+02, 2.2995e+06],\n",
            "       [1.6400e+02, 2.6463e+06],\n",
            "       [1.6321e+02, 1.3675e+06],\n",
            "       [1.6480e+02, 1.2173e+06],\n",
            "       [1.6794e+02, 1.2216e+06],\n",
            "       [1.6429e+02, 1.8486e+06],\n",
            "       [1.6640e+02, 2.0891e+06],\n",
            "       [1.6651e+02, 1.9968e+06],\n",
            "       [1.7000e+02, 5.0068e+06],\n",
            "       [1.7080e+02, 1.7690e+06],\n",
            "       [1.6956e+02, 1.8985e+06],\n",
            "       [1.7206e+02, 1.8241e+06],\n",
            "       [1.7203e+02, 1.2034e+06],\n",
            "       [1.7203e+02, 1.6162e+06],\n",
            "       [1.7466e+02, 1.3752e+06],\n",
            "       [1.7475e+02, 1.4866e+06],\n",
            "       [1.7368e+02, 1.2271e+06],\n",
            "       [1.7500e+02, 2.4787e+06],\n",
            "       [1.7220e+02, 2.1024e+06],\n",
            "       [1.7444e+02, 1.4331e+06],\n",
            "       [1.7230e+02, 1.5865e+06]], dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
            "array([[164.59],\n",
            "       [163.99],\n",
            "       [165.38],\n",
            "       [164.44],\n",
            "       [164.07],\n",
            "       [165.21],\n",
            "       [165.96],\n",
            "       [165.19],\n",
            "       [166.41],\n",
            "       [166.26],\n",
            "       [168.68],\n",
            "       [165.5 ],\n",
            "       [163.25],\n",
            "       [164.83],\n",
            "       [167.69],\n",
            "       [166.55],\n",
            "       [164.97],\n",
            "       [166.43],\n",
            "       [167.8 ],\n",
            "       [170.95],\n",
            "       [168.93],\n",
            "       [172.64],\n",
            "       [171.06],\n",
            "       [172.14],\n",
            "       [173.92],\n",
            "       [174.03],\n",
            "       [175.15],\n",
            "       [175.  ],\n",
            "       [172.11],\n",
            "       [173.11],\n",
            "       [173.35],\n",
            "       [174.11]], dtype=float32)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple model for stock price prediction (e.g., linear regression)\n",
        "def create_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(2,)),\n",
        "      tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "      tf.keras.layers.Dense(10, kernel_initializer='zeros')\n",
        "  ])\n",
        "\n",
        "    # Debug: Check model summary\n",
        "    #print(model.summary())\n",
        "    return model\n",
        "\n",
        "create_model()"
      ],
      "metadata": {
        "id": "kOmx0SkP6yky",
        "outputId": "75f2c474-c951-4c22-ba59-ef55b6027ee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x7d9dec4dfe50>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a TFF model\n",
        "def model_fn():\n",
        "    try:\n",
        "        # Create the Keras model using the function defined earlier\n",
        "        keras_model = create_model()\n",
        "\n",
        "        # Ensure input_spec matches the dataset structure (features, labels)\n",
        "        input_spec = (\n",
        "            tf.TensorSpec(shape=[None, 2], dtype=tf.float32),  # Features (2: Close price, Volume)\n",
        "            tf.TensorSpec(shape=[None, 1], dtype=tf.float32)   # Labels (1: Close price prediction)\n",
        "        )\n",
        "\n",
        "        # Create a TFF model from the Keras model\n",
        "        tff_model = tff.learning.models.from_keras_model(\n",
        "            keras_model=keras_model,\n",
        "            input_spec=input_spec,\n",
        "            loss=tf.keras.losses.Huber(),  # Loss function (regression task)\n",
        "            metrics=[tf.keras.metrics.MeanSquaredError()]  # Metrics to track\n",
        "        )\n",
        "\n",
        "        print(\"TFF model created successfully.\")\n",
        "\n",
        "        return tff_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while creating TFF model: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Federated learning process setup\n",
        "def federated_training():\n",
        "    client_data = create_client_data()\n",
        "\n",
        "    # Use TFF's internal sgdm optimizer instead of Keras's optimizer\n",
        "    client_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=0.1)\n",
        "    server_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=0.9)\n",
        "\n",
        "    # Build federated averaging process using TFF's internal optimizers\n",
        "    iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn=model_fn,\n",
        "        client_optimizer_fn=client_optimizer_fn,  # Pass the optimizer directly\n",
        "        server_optimizer_fn=server_optimizer_fn   # Pass the optimizer directly\n",
        "    )\n",
        "\n",
        "    print(iterative_process.initialize.type_signature.formatted_representation())\n",
        "\n",
        "    # Initialize the model state\n",
        "    state = iterative_process.initialize()\n",
        "\n",
        "    print('Initial State....')\n",
        "    print(state.distributor)\n",
        "\n",
        "    # Simulate training across clients (5 companies)\n",
        "    for round_num in range(1, 30):\n",
        "        state, metrics = iterative_process.next(state, client_data)\n",
        "        if round_num==29:\n",
        "          print(state)\n",
        "        print(f'Round {round_num}, Metrics: {metrics}')\n",
        "\n",
        "# Start federated training\n",
        "federated_training()"
      ],
      "metadata": {
        "id": "0ZqcHIFP6yLh",
        "outputId": "9b1d05b3-6c17-4a62-cd3e-ecdc6a875476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFF model created successfully.\n",
            "TFF model created successfully.\n",
            "TFF model created successfully.\n",
            "( -> <\n",
            "  global_model_weights=<\n",
            "    trainable=<\n",
            "      float32[2,10],\n",
            "      float32[10],\n",
            "      float32[10,10],\n",
            "      float32[10]\n",
            "    >,\n",
            "    non_trainable=<>\n",
            "  >,\n",
            "  distributor=<>,\n",
            "  client_work=<\n",
            "    learning_rate=float32\n",
            "  >,\n",
            "  aggregator=<\n",
            "    value_sum_process=<>,\n",
            "    weight_sum_process=<>\n",
            "  >,\n",
            "  finalizer=<\n",
            "    learning_rate=float32\n",
            "  >\n",
            ">@SERVER)\n",
            "Initial State....\n",
            "()\n",
            "Round 1, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 113929.445), ('loss', 324.53735), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 2, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 113695.555), ('loss', 324.17737), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 3, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 113461.93), ('loss', 323.81735), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 4, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 113228.55), ('loss', 323.45734), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 5, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 112995.44), ('loss', 323.09735), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 6, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 112762.555), ('loss', 322.73734), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 7, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 112529.96), ('loss', 322.37735), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 8, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 112297.625), ('loss', 322.01733), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 9, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 112065.53), ('loss', 321.65738), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 10, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 111833.73), ('loss', 321.29733), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 11, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 111602.14), ('loss', 320.93735), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 12, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 111370.86), ('loss', 320.57736), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 13, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 111139.78), ('loss', 320.21732), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 14, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 110909.01), ('loss', 319.85733), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 15, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 110678.44), ('loss', 319.4973), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 16, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 110448.18), ('loss', 319.1373), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 17, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 110218.17), ('loss', 318.77728), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 18, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 109988.414), ('loss', 318.4173), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 19, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 109758.92), ('loss', 318.05728), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 20, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 109529.67), ('loss', 317.69727), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 21, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 109300.72), ('loss', 317.33728), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 22, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 109071.984), ('loss', 316.97726), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 23, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 108843.516), ('loss', 316.61722), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 24, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 108615.32), ('loss', 316.25726), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 25, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 108387.38), ('loss', 315.8972), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 26, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 108159.71), ('loss', 315.53723), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 27, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 107932.28), ('loss', 315.17725), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 28, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 107705.14), ('loss', 314.8172), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "LearningAlgorithmState(global_model_weights=ModelWeights(trainable=[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), array([10.440144, 10.440144, 10.440144, 10.440144, 10.440144, 10.440144,\n",
            "       10.440144, 10.440144, 10.440144, 10.440144], dtype=float32)], non_trainable=[]), distributor=(), client_work=OrderedDict([('learning_rate', 0.1)]), aggregator=OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())]), finalizer=OrderedDict([('learning_rate', 0.9)]))\n",
            "Round 29, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 107478.234), ('loss', 314.45724), ('num_examples', 1257), ('num_batches', 40)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8YJfXUG6x-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRtQMfxH84RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9aOmQhqo83l1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}